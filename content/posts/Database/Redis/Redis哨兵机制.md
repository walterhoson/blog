---
title: Redis 哨兵机制
description: 主要分析 Redis 哨兵机制的原理，以及如何解决主库单点问题。
toc: true
authors: 
    - WayneShen
tags: 
    - Redis
categories: 
    - DataBase
series: []
date: '2021-07-31T16:30:+08:00'
lastmod: '2021-07-31T16:40:20+08:00'
draft: false
---

</br>

主要分析 Redis 哨兵机制的原理，以及如何解决主库单点问题。

<!--more-->

在主从集群模式下，若从库发生了故障，客户端可以继续向主库或其他从库发送请求，进行相关的操作，但若主库发生了故障，那就直接会影响到从库的同步。

虽然从库仍可以提供读操作，纯读的操作还能接受主库故障，但若有写操作，按照主从读写分离，主库此时无法提供写操作，就无法接受了。

无论是写服务中断，还是从库无法进行数据同步，都是无法接受的。所以主库挂了，就需要运行一个新主库，比如把一个从库切换为主库，此时涉及到三个问题：

1. 主库是否真的挂了？
2. 该选择哪个从库作为主库？
3. 如何把新主库的相关信息通知给从库和客户端？

## 哨兵机制

Redis 主从集群中，**哨兵机制是实现主从库自动切换的关键机制**，有效**解决了主从复制模式下故障转移**的这三个问题。

### 基本流程

**哨兵其实就是一个运行在特殊模式下的 Redis 进程**，主从库实例运行的同时，它也在运行。哨兵主要负责的就是三个任务：**监控、选主（选择主库）和通知**。

#### 监控

监控是指**哨兵进程在运行时，周期性地给所有的主从库发送 PING 命令，检测它们是否仍然在线运行**。

若从库没有在规定时间内响应哨兵的 PING 命令，哨兵就会把它标记为“下线状态”；同样，若主库也没有在规定时间内响应哨兵的 PING 命令，哨兵就会判定主库下线，然后开始**自动切换主库**的流程。

#### 选主

哨兵的第二个任务就是选主，主库宕机后，哨兵就需要从多个从库中，按照一定的规则选择一个从库实例，将其作为新的主库。

#### 通知

在执行通知任务时，哨兵会把新主库的连接信息发给其他从库，让它们执行 replicaof 命令，和新主库建立连接，并进行数据复制。同时，哨兵会把新主库的连接信息通知给客户端，让它们把请求操作发到新主库上。

![notice](../../../assets/Redis哨兵机制/notice.jpg)

在监控和选主这两个任务中，哨兵需要做出两个决策：

- 在监控任务中，哨兵需要判断主库是否处于下线状态；
- 在选主任务中，哨兵也要决定选择哪个从库实例作为主库。

哨兵对主库的下线判断有“主观下线”和“客观下线”两种。

### 主观和客观下线

#### 主观下线

**哨兵进程会使用 PING 命令检测它自己和主、从库的网络连接情况，用来判断实例的状态**。若哨兵发现主库或从库对 PING 命令的响应超时了，那哨兵就会先把它标记为“主观下线”。

若检测的是从库，那哨兵简单地将其标记为“主观下线”就行了，因为从库的下线影响一般不太大，集群的对外服务不会间断。

但若检测的是主库，那哨兵还不能简单地把它标记为“主观下线”。因为会存在哨兵误判的情况，一般误判发生在集群网络压力比较大、网络拥塞，或者是主库本身压力比较大的情况下。若误判，而主库实际并没有故障就开启主从切换的话，会产生后续选主和通知操作带来的额外的计算和通信开销。

为了减少误判，**通常会采用多实例组成的集群模式进行部署，这也被称为哨兵集群**，引入多个哨兵实例一起判断，因为多个哨兵的网络同时不稳定的概率比较小，误判率就可以降低。

只有大多数哨兵实例都判断主库已经“主观下线”了，主库才会被标记为“客观下线”。原则就是：少数服从多数。简单而言，当有 N 个哨兵实例，最好有 `N / 2 + 1` 个哨兵判断主库为“主观下线”，才能最终判定为主库“客观下线”。（有多少个实例做出“主观下线”的判断，可以由 Redis 管理员自行设定）。

### 如何选新主库

哨兵选新主库的过程称为“筛选+打分”。简单而言，在多个从库中，先按照**一定的筛选条件**，把不符合条件的从库去掉。然后，再按照**一定的规则**，给剩下的从库逐个打分，将得分最高的从库选为新主库。

![choose-master](../../../assets/Redis哨兵机制/choose-master.jpg)

#### 筛选的条件

一般情况下，肯定要先保证所选的从库仍然在线运行。不过在选主时从库正常在线只能表示从库的现状良好，并不代表它就最适合。

**除了要检查从库的当前在线状态，还要判断它之前的网络连接状态**。若从库总是和主库断连，且断连次数超出了一定的阈值，就有理由相信这个从库的网络状况并不是太好，就可以把这个从库筛掉了。

使用配置项 `down-after-milliseconds * 10`。其中，**`down-after-milliseconds` 是认定主从库断连的最大连接超时时间**。若在 `down-after-milliseconds` 毫秒内，主从节点都没有通过网络联系上，就可以认为主从节点断连了。若发生断连的次数超过了 10 次，就说明这个从库的网络状况不好，不适合作为新主库。

#### 打分

给筛选完后剩余的从库进行打分，可以分别按照三个规则以此进行三轮打分，这三个规则分别是**从库优先级、从库复制进度以及从库 ID 号**。

只要在某一轮中，有从库得分最高，那么它就是主库了，选主过程到此结束。若没有出现得分最高的从库，那么就继续进行下一轮。

**第一轮：优先级最高的从库得分高**

用户可以通过 `slave-priority` 配置项，给不同的从库设置不同优先级。比如，有两个从库，它们的内存大小不一样，可以手动给内存大的实例设置一个高优先级。在选主时，哨兵会给优先级高的从库打高分，若有一个从库优先级最高，那么它就是新主库了。

若从库的优先级都一样，那么哨兵开始第二轮打分。

**第二轮：和旧主库同步程度最接近的从库得分高**

改规则的依据是，若选择和旧主库同步最接近的那个从库作为主库，那么，这个新主库上就有最新的数据。

此时就需要判断从库和旧主库间的同步进度，主从库再同步时有个命令传播的过程。在该过程中，主库会使用 master_repl_offset 记录当前的最新写操作在 repl_backlog_buffer 中的位置，而从库会用 slave_repl_offset 这个值记录当前的复制进度。此时剩余从库中的 slave_repl_offset 最接近 master_repl_offset 的从库就得分最高，可以作为新主库。

若两个从库的 slave_repl_offset 值大小一致，就需要对它们进行第三轮打分了。

**第三轮：ID 号小的从库得分高**

每个实例都会有一个 ID，这个 ID 就类似这里的从库编号。目前，Redis 在选主库时，有一个默认的规定：**在优先级和复制进度都相同的情况下，ID 号最小的从库得分最高，会被选为新主库**。

至此，选出了新的主库，选主就完成了。

所以整个流程中，首先，哨兵会按照在线状态、网络状态，筛选过滤掉一部分不符合要求的从库；然后，依次按照优先级、复制进度、ID 号大小再对剩余的从库进行打分，只要有得分最高的从库出现，就把它选为新主库。

## 哨兵机制总结

哨兵机制是实现 Redis 不间断服务的重要保证。具体来说，主从集群的数据同步，是数据可靠的基础保证；而在主库发生故障时，自动的主从切换是服务不间断的关键支撑。

Redis 的哨兵机制自动完成了以下三大功能，从而实现了主从库的自动切换，可以降低 Redis 集群的运维开销：

- 监控主库运行状态，并判断主库是否客观下线；
- 在主库客观下线后，选取新主库；
- 选出新主库后，通知从库和客户端。

为了降低误判率，在实际应用时，哨兵机制通常采用多实例的方式进行部署，多个哨兵实例通过“少数服从多数”的原则，来判断主库是否客观下线。一般来说，可以部署三个哨兵，如果有两个哨兵认定主库“主观下线”，就可以开始切换过程。当然，若希望进一步提升判断准确率，也可以再适当增加哨兵个数，比如说使用五个哨兵。

但使用多个哨兵实例来降低误判率，其实相当于组成了一个哨兵集群，也会因此面临着一些新的挑战，例如：

- 哨兵集群中有实例挂了，怎么办，会影响主库状态判断和选主吗？
- 哨兵集群多数实例达成共识，判断出主库“客观下线”后，由哪个实例来执行主从切换呢？

要搞懂这些问题，就不得不提哨兵集群了。

## 哨兵集群

Redis 通过哨兵集群解决哨兵宕机主库切换问题。

哨兵机制实现了主从库的自动切换。部署多个实例，就形成了一个哨兵集群。哨兵集群中的多个实例共同判断，可以降低对主库下线的误判率。

此时主要的问题就是，若有哨兵实例在运行时发生了故障，如何保证主从库之间的正常切换。

实际上，一旦多个实例组成了哨兵集群，即使有哨兵实例出现故障挂了，其他哨兵还能继续协作完成主从库切换的工作。

部署哨兵集群时，需要配置哨兵的信息，需要用到以下配置项，设置**主库的 IP **和**端口**，而不需要配置其他哨兵的连接信息：

```shell
sentinel monitor <master-name> <ip> <redis-port> <quorum>
```

## 集群组成原理和运行机制

### 基于 pub/sub 机制的哨兵集群组成

哨兵实例之间可以相互发现，用到了 Redis 提供的 pub/sub 机制（发布/订阅机制）。哨兵只要和主库建立起了连接，就可以在主库上发布消息了，比如发布自己的连接信息（IP 和端口）。同时，它也可以从主库上订阅消息，获得其他哨兵发布的连接信息。当多个哨兵实例都在主库上做了发布和订阅操作后，它们之间就能知道彼此的 IP 地址和端口。

除了哨兵实例，开发的应用程序也可通过 Redis 进行消息的发布和订阅。为了区分不同应用的消息，Redis 会以频道（消息的类别）的形式来管理不同的类别。

**只有订阅了同一个频道的应用，才能通过发布的消息进行信息交换**。

在主从集群中，主库上有一个名为“`__sentinel__:hello`”的频道，不同哨兵就是通过它来相互发现，实现互相通信的。

![pub_sub](../../../assets/Redis哨兵机制/pub_sub.jpg)

哨兵除了彼此之间建立起连接形成集群外，还需要和从库建立连接。因为，在哨兵的监控任务中，它需要对主从库都进行心跳判断，而且在主从库切换完成后，它还需要通知从库，让它们和新主库进行同步。

**哨兵通过向主库发送 INFO 命令来获取从库的 IP 地址和端口信息**，主库接受到该命令后，就会把从库列表返回给哨兵，接着哨兵就可以根据从库列表中的连接信息与每个从库建立连接，并在该连接上持续地对从库进行监控。

![sentinel_slave](../../../assets/Redis哨兵机制/sentinel_slave.jpg)

哨兵不能只和主、从库连接。因为，主从库切换后，客户端也需要知道新主库的连接信息，才能向新主库发送请求操作。所以，哨兵还需要完成把新主库的信息告诉客户端这个任务。

此时，依赖 pub/sub 机制，来帮助完成哨兵和客户端间的信息同步。比如客户端需要能够获取到哨兵集群在监控、选主、切换这个过程中发生的各种事件。

### 基于 pub/sub 机制的客户端事件通知

本质上，哨兵就是一个运行在特定模式下的 Redis 实例，只不过它并不服务请求操作，只是完成监控、选主和通知的任务。所以，每个哨兵实例也提供 pub/sub 机制，客户端可以从哨兵订阅消息。哨兵提供的消息订阅频道有很多，不同频道包含了主从库切换过程中的不同关键事件。

重要的频道以及涉及的几个关键事件如下，包括主库下线判断、新主库选定、从库重新配置。

![channel](../../../assets/Redis哨兵机制/channel.jpg)

客户端可以通过这些频道从哨兵这里订阅消息，具体步骤是，客户端读取哨兵的配置文件后，可以获得哨兵的地址和端口，和哨兵建立网络连接。然后可以在客户端执行订阅命令，来获取不同的消息。

举个例子：

执行如下命令，来订阅“所有实例进入客观下线状态的事件”：

```shell
SUBSCRIBE +odown
```

执行如下命令，订阅所有事件

```shell
PSUBSCRIBE  *
```

当哨兵把新主库选出来后，客户端就会看到以下的 switch-master 事件，该事件表示主库已经切换了，新主库的 IP 地址和端口信息已经有了。此时，客户端就可以用这里面的新主库地址和端口进行通信了。

```shell
switch-master <master name> <oldip> <oldport> <newip> <newport>
```

有了这些事件通知，客户端不仅可以在主从切换后得到新主库的连接信息，还可以监控到主从库切换过程中发生的各个重要事件。这样，客户端就可以知道主从切换进行到哪一步了，有助于了解切换进度。

有了 pub/sub 机制，哨兵和哨兵之间、哨兵和从库之间、哨兵和客户端之间就都能建立起连接了，再加上主库下线判断和选主依据，哨兵集群的监控、选主和通知三个任务就基本可以正常工作了。

此时还有个问题，主库故障以后，如何确定哨兵集群中的哪个哨兵来进行实际的主从切换？

### 选择哨兵执行主从切换

与主库“客观下线“过程类似，需要进行一个投票仲裁的过程。

任何一个实例只要自身判断主库“主观下线”后，就会给其他实例发送 `is-master-down-by-addr` 命令。接着，其他实例会根据自己和主库的连接情况，做出 Y 或 N 的响应，Y 相当于赞成票，N 相当于反对票。

![master_switch](../../../assets/Redis哨兵机制/master_switch.jpg)

一个哨兵获得了仲裁所需的赞成票数后，就可以标记主库为“客观下线”。这个所需的赞成票数是通过哨兵配置文件中的 quorum 配置项设定。例如，现在有 5 个哨兵，quorum 配置的是 3，那么一个哨兵需要 3 张赞成票，就可以标记主库为“客观下线”了。这 3 张赞成票包括哨兵自己的一张赞成票和另外两个哨兵的赞成票。

此时这个哨兵就可以再给其他哨兵发送命令，表明希望由自己来执行主从切换，并让所有其他哨兵进行投票。这个投票过程称为”Leader 选举“，最终执行主从切换的哨兵称为 Leader，投票的过程就是确定 Leader。

投票过程中，任何一个想称为 Leader 的哨兵，要满足两个条件：

1. 拿到半数以上的赞成票
2. 拿到的票数需要大于等于哨兵配置文件中的 quorum 值

以三个哨兵为例，假设此时的 quorum 设置为 2，那么任何一个想成为 Leader 的哨兵只要拿到 2 张赞成票。

![quorum_process](../../../assets/Redis哨兵机制/quorum_process.jpg)

在 T1 时刻，S1 判断主库为“客观下线”，它想成为 Leader，就先给自己投一张赞成票，然后分别向 S2 和 S3 发送命令，表示要成为 Leader。

在 T2 时刻，S3 判断主库为“客观下线”，也想成为 Leader，所以也先给自己投一张赞成票，再分别向 S1 和 S2 发送命令，表示要成为 Leader。

在 T3 时刻，S1 收到了 S3 的 Leader 投票请求。因为 S1 已经给自己投了一票 Y，所以它不能再给其他哨兵投赞成票了，所以 S1 回复 N 表示不同意。同时，S2 收到了 T2 时 S3 发送的 Leader 投票请求。因为 S2 之前没有投过票，它会给第一个向它发送投票请求的哨兵回复 Y，给后续再发送投票请求的哨兵回复 N，所以，在 T3 时，S2 回复 S3，同意 S3 成为 Leader。

在 T4 时刻，S2 才收到 T1 时 S1 发送的投票命令。因为 S2 已经在 T3 时同意了 S3 的投票请求，此时，S2 给 S1 回复 N，不同意 S1 成为 Leader。发生这种情况是因为 S3 和 S2 之间的网络传输正常，而 S1 和 S2 之间的网络传输可能正好拥塞了，导致投票请求传输慢了。

最后，在 T5 时刻，S1 得到的票数是来自它自己的一票 Y 和来自 S2 的一票 N。而 S3 除了自己的赞成票 Y 以外，还收到了来自 S2 的一票 Y。此时，S3 不仅获得了半数以上的 Leader 赞成票，也达到预设的 quorum 值（quorum 为 2），所以它最终成为了 Leader。接着，S3 会开始执行选主操作，而且在选定新主库后，会给其他从库和客户端通知新主库的信息。

若 S3 没有拿到 2 票 Y，那么这轮投票就不会产生 Leader。哨兵集群会等待一段时间（哨兵故障转移超时时间的 2 倍），再重新选举。这是因为，**哨兵集群能够进行成功投票，很大程度上依赖于选举命令的正常网络传播**。若网络压力较大或有短时堵塞，就可能导致没有一个哨兵能拿到半数以上的赞成票。所以，等到网络拥塞好转之后，再进行投票选举，成功的概率就会增加。

值得注意的是，若哨兵集群只有两个实例，此时一个哨兵想称为 Leader，必须获得 2 票，而不是 1 票，所以如果有个哨兵挂了，那么，此时的集群是无法进行主从库切换的。因此，通常至少会配置 3 个哨兵实例。

## 哨兵集群总结

为了实现主从切换，引入了哨兵；为了避免单个哨兵故障后无法进行主从切换，以及减少误判率，又引入了哨兵集群；哨兵集群又需要有一些机制来支撑它的正常运行。

支持哨兵集群的这些关键机制，包括：

- 基于 pub/sub 机制的哨兵集群组成过程；
- 基于 INFO 命令的从库列表，这可以帮助哨兵和从库建立连接；
- 基于哨兵自身的 pub/sub 功能，这实现了客户端和哨兵之间的事件通知。

对于主从切换，并非一个哨兵想执行就可以执行，需要哨兵集群在判断了主库“客观下线”后，经过投票仲裁，选举一个 Leader 出来，由它负责实际的主从切换，即由它来完成新主库的选择以及通知从库与客户端。

最后，分享一个经验：**要保证所有哨兵实例的配置是一致的，尤其是主观下线的判断值 down-after-milliseconds**。曾经就踩过一个“坑”。当时项目中，因为这个值在不同的哨兵实例上配置不一致，导致哨兵集群一直没有对有故障的主库形成共识，也就没有及时切换主库，最终的结果就是集群服务不稳定。所以，一定不要忽略这条看似简单的经验。

## 参考资料

《Redis 设计与实现》

《Redis 核心技术与实战》